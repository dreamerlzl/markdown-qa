# Project Context

## Purpose
A local markdown Q&A system that enables question-answering over markdown documentation files using retrieval-augmented generation (RAG). The system runs as a long-running server process that maintains vector indexes in memory for fast query responses, with support for incremental index updates when files change.

## Tech Stack
- Python 3.13+
- LangChain (for text splitting and embeddings)
- FAISS (vector database for similarity search)
- OpenAI-compatible API client (for embeddings and LLM)
- WebSockets (for client-server communication)
- PyYAML/TOML (for configuration files)
- Watchdog (for configuration file watching)
- pytest (for testing)

## Project Conventions

### Code Style
- Type hints are used throughout
- Docstrings follow Google style
- Module-level organization with clear separation of concerns
- Error handling with appropriate exception types

### Architecture Patterns
- Server-client architecture with WebSocket communication
- In-memory index management with atomic swapping
- Incremental index updates to minimize processing overhead
- Configuration management with file watching and hot reload
- Embedding caching to avoid redundant API calls
- Manifest-based tracking of indexed files and metadata

### Testing Strategy
- Unit tests for individual modules
- Integration tests for server-client interactions
- pytest with asyncio support for async code
- Test files located in `tests/` directory

### Git Workflow
- Standard git workflow with commits and branches
- No specific branching strategy enforced

## Domain Context
The system is designed for local question-answering over markdown documentation. Key concepts:
- **Index**: A vector store containing embeddings of markdown file chunks
- **Chunk**: A segment of markdown content with associated metadata
- **Manifest**: A JSON file tracking which directories are indexed and per-file metadata
- **Incremental Update**: Updating only changed files in an index rather than rebuilding everything
- **Streaming Response**: Sending answer chunks as they are generated by the LLM

## Important Constraints
- Server must maintain indexes in memory for fast queries when directories are configured
- Server may start with no directories configured (no indexed content; status reflects this)
- Index updates should not block query processing
- Configuration changes should be applied without server restart when possible
- API configuration (base URL, API key) is required for embeddings and LLM
- Default cache directory is `~/.md-qa/cache/`
- Default config directory is `~/.md-qa/`

## External Dependencies
- OpenAI-compatible API endpoint (for embeddings and LLM generation)
- FAISS library (for vector similarity search)
- LangChain text splitters (for markdown chunking)
